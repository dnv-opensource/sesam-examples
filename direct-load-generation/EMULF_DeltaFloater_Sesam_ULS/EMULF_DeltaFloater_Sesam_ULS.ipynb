{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook example demonstrates how to run a model of the EMULF Delta Floater, exposed to time-dependent wave loads coming from Sima coupled analysis results via load reconstruction in Wasim, using OneWorkflow locally or in the cloud (**note**: cloud support will be added soon!)\n",
    "\n",
    "Highlighted features of this example are:\n",
    "- Read control data from Excel spreadsheet into a dictionary (Python), including file names for coupled analysis result, as well as start and stop times for Wasim load reconstruction and load transfer. More control data can easily be added to the spreadsheet and transferred to relevant input files using [`CreateInputFileFromFileTemplateCommand` (OneWorkflow)](https://myworkspace.dnv.com/download/public/sesam/workflow/docs/latest/source/dnv.oneworkflow.html#dnv.oneworkflow.create_input_file_from_file_template_command.CreateInputFileFromFileTemplateCommand). Some template control data are hardcoded by user in the notebook.\n",
    "- Apply safety load factors. Load factors are first read from the spreadsheet, then modified and applied to the loads resulting from two Wasim runs, where one (total) loadcase includes all load effects and one (calm sea) load case includes only permanent loads. A single Sestra analysis is then run using the combined loads. The modified factors are applied to separate L#-files and then combined in sestra.inp using the MULL card. This is \n",
    "- One row set up for short time period load reconstruction and/or short time period load transfer only. Repeat rows for multiple events and/or seeds.\n",
    "- Threshold value limiting output of usage factor results. See options in [`SesamCoreCommand`](https://myworkspace.dnv.com/download/public/sesam/workflow/docs/latest/source/dnv.sesam.commands.html#dnv.sesam.commands.sesam_core_command.SesamCoreCommand).\n",
    "- Control Sesam Core run using commands in journal (JNL) file.\n",
    "- Graphs of reaction force in z-direction from Sestra are presented for each load case.\n",
    "- Buckling results are presented as a table of worst usage factor over all time steps and load cases for each stiffened plate field. Note that the table omits the two first time steps, to avoid unrealistic result peaks.\n",
    "- Buckling results are also shown for a user selected number of graphs starting from a user selected usage factor as time-history plots of usage factor of all stiffeners in each stiffened plate field. \n",
    "\n",
    "Work in progress:\n",
    "- To be added: Separate start and stop of buckling calculation in Sesam Core command journal file . Sestra is run for all time steps with load mapping.\n",
    "\n",
    "The capacity model comes as a single JSON file from GeniE, and results are currently stored in one .csv and one .lis file for each stiffened plate field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EMULF Deltafloater](EMULF_Deltafloater.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneWorkflow setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specify <font color='red'>root_folder / workspaceId</font>.\n",
    "- Specify <font color='red'>oneWorkflowTMPFolder</font>.\n",
    "- Use <font color='red'>cloudRun</font> variable to control running locally or in the cloud. (**note**: Let this be `False` until cloud support has been added and you have a valid cloud subscription.)\n",
    "- Specify Excel file with load case names, start/stop/time step info and safety load factors for the input_data variable (<font color='red'>control_input2.xlsx</font>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "sys.path.append(\"../PythonModules\")\n",
    "root_folder = os.getcwd()\n",
    "\n",
    "#User defined setup, see comments above\n",
    "workspacePath = str(Path(root_folder, 'Workspace'))\n",
    "workspaceId = \"EMULF_DeltaFloater_Sesam_ULS\"\n",
    "cloudRun = False\n",
    "input_data = pd.read_excel(os.path.join(workspacePath, \"control_input2.xlsx\"), index_col=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnv.oneworkflow.utils import *\n",
    "# local workspace, all results will be put here in the folder Loadcases, after local or cloud runs\n",
    "# location of common files for all analysis, has to be below workspacePath and in the folder names CommonFiles\n",
    "# If running locally the code below will also start the local workflow host.\n",
    "workflow_client = one_workflow_client(workspace_id = workspaceId, cloud_run = cloudRun, workspace_path = workspacePath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Excel input, update template parameters in .inp files, run Wasim and Sesam Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files and file locations:\n",
    "- Sima coupled analysis results files in the *LoadCases\\\\<load_case_name_from_excel>\\\\* folder. Results for a separate calm sea load case is required for the permanent load factor G.\n",
    "- All other model and input, including template files are found in the *CommonFiles* folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove preivous existing resulte before running the analysis\n",
    "import shutil\n",
    "clean = True\n",
    "if clean:\n",
    "    shutil.rmtree(os.path.join(workspacePath,\"LoadCases\"), ignore_errors=True)\n",
    "shutil.copytree(os.path.join(workspacePath,\"Input\"),os.path.join(workspacePath),dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import os\n",
    "from dnv.sesam.commands import *\n",
    "from dnv.oneworkflow import *\n",
    "import shutil\n",
    "from WasimTaskCreator import *\n",
    "import json\n",
    "\n",
    "# Template parameters hardcoded by user\n",
    "topsuper = 1 #top superelement number\n",
    "prefix_stru = \"STR_\" #prefix for structure files\n",
    "fixed_parameters_all_load_cases = {\n",
    "    'mor_topsel': topsuper,\n",
    "    'prefix': f\"{prefix_stru}        \", #prefix, left-justified\n",
    "    'prefix_stru': prefix_stru, #prefix, right-justified\n",
    "    'prefix_struCalmSea': f'{prefix_stru}CalmSea',\n",
    "    'prefix_struTotal' : f'{prefix_stru}Total'\n",
    "} \n",
    "\n",
    "\n",
    "try:\n",
    "    os.chdir(os.path.join(workspacePath,\"LoadCases\"))\n",
    "except:\n",
    "    print(\"LoadCases folder not found\")\n",
    "workflow_sequence = []\n",
    "shutil.copytree(os.path.join(workspacePath,\"Input\"),os.path.join(workspacePath),dirs_exist_ok=True)\n",
    "\n",
    "#Recognize the following column titles in Excel spreadsheet and map them to the correct input parameters given in Wasim template files\n",
    "parameter_mapping = {'Group': \"group\",\n",
    "\t\t\t\t'LoadCase': \"loadcase\",\n",
    "\t\t\t\t\"SimaFilesFolder\": \"sima_files_folder\",\n",
    "\t\t\t\t\"SimaFilesName\": \"sima_files_name\",\n",
    "                \"TimeStep\": \"timestep\",\n",
    "\t\t\t\t\"Depth\": \"depth\",\n",
    "\t\t\t\t\"StartWasimSolve\": \"start_solve\",\n",
    "\t\t\t\t\"StopWasimSolve\": \"stop_solve\",\n",
    "            \t\"StartWasimStru\": \"start_stru\",\n",
    "\t\t\t\t\"StopWasimStru\": \"stop_stru\",\n",
    "\t\t\t\t\"NSteps\": \"nsteps\",\n",
    "\t\t\t\t\"StartSestra\": \"start_sestra\",\n",
    "\t\t\t\t\"StopSestra\": \"stop_sestra\",\n",
    "\t\t\t\t\"StartBuckling\": \"BUCKLINGSTART\",\n",
    "\t\t\t\t\"StopBuckling\": \"BUCKLINGEND\",\n",
    "\t\t\t\t\"LoadFactorSet\": \"loadfactorset\",\n",
    "                \"fac_G\": \"fac_g\",\n",
    "                \"fac_E\": \"fac_e\",\n",
    "                }\n",
    "\n",
    "#Display data read from Excel\n",
    "display(input_data)\n",
    "\n",
    "#Loop over all load cases\n",
    "for casename, case in input_data.iterrows():\n",
    "    casedict = case.to_dict()\n",
    "    input_parameters = {}\n",
    "    for key, value in case.items():\n",
    "        input_parameters[parameter_mapping[key]] = str(value)\n",
    "    \n",
    "    #Calculate modified load factors since Wasim doesn't distinguish static and dynamic loads\n",
    "    input_parameters[\"fac_dyn\"] = float(input_parameters['fac_e'])\n",
    "    input_parameters[\"fac_calm\"] = float(input_parameters['fac_g']) - float(input_parameters['fac_e'])\n",
    "\n",
    "    input_parameters = input_parameters | fixed_parameters_all_load_cases\n",
    "\n",
    "    print(\"The following parameters are used for load case: \" + casename)\n",
    "    print(json.dumps(input_parameters, indent=4, sort_keys=True))\n",
    "\n",
    "    #Run Wasim including update of .inp files, for ULS including load factors\n",
    "    tasks = WasimTaskCreator().CreateTasks(input_parameters)\n",
    "    calm_sea_tasks =  WasimTaskCreator().CreateTasks(input_parameters, \"CalmSea\")\n",
    "    tasks.extend(calm_sea_tasks)\n",
    "    #Apply load factors\n",
    "    fac_dyn = input_parameters[\"fac_dyn\"]\n",
    "    fac_calm=input_parameters[\"fac_calm\"]\n",
    "    # Update S#-files created by Wasim_stru, edit LCOM and append SCAL card\n",
    "    scaling_command_dyn = PythonCommand(\n",
    "            directory=workflow_client.common_directory,\n",
    "            filename=\"appendScalingFactor.py\",\n",
    "            args=f\"{prefix_stru}TotalS1.FEM {fac_dyn}\"\n",
    "            )\n",
    "    tasks.append(scaling_command_dyn)\n",
    "\n",
    "    scaling_command_calm = PythonCommand(\n",
    "            directory=workflow_client.common_directory,\n",
    "            filename=\"appendScalingFactor.py\",\n",
    "            args=f\"{prefix_stru}CalmSeaS1.FEM {fac_calm}\"\n",
    "            )\n",
    "    tasks.append(scaling_command_calm)\n",
    "    #Update Sestra .inp file\n",
    "    sestra_template_command = CreateInputFileFromFileTemplateCommand(\n",
    "        template_input_file  = \"sestra_template.inp\",\n",
    "        input_filename  = \"sestra.inp\",\n",
    "        parameters= input_parameters\n",
    "    )\n",
    "    tasks.append(sestra_template_command)    \n",
    "    \n",
    "    #Update Sesam Core .jnl file\n",
    "    sesam_core_template_command = CreateInputFileFromFileTemplateCommand(\n",
    "        template_input_file  = \"SesamCore_buckling_template.jnl\",\n",
    "        input_filename  = \"SesamCore_buckling.jnl\",\n",
    "        parameters= input_parameters\n",
    "    )\n",
    "    tasks.append(sesam_core_template_command)\n",
    "    \n",
    "    #Run Sesam Core\n",
    "    tasks.append(SesamCoreCommand(command = \"ULS\",input_file_name= \"input.json\", options = \"-v\"))\n",
    "    \n",
    "    #Run the above defined sequence of tasks\n",
    "    workflow_sequence.append(CommandInfo(commands=tasks,load_case_foldername=casename))\n",
    "    \n",
    "    \n",
    "print(\"Running commands in parallel\")\n",
    "await run_managed_commands_in_parallel_async(\n",
    "             client=workflow_client,\n",
    "             commands_info=workflow_sequence,\n",
    "             log_job=False,\n",
    "             files_to_download_from_blob_to_client=FileTransferOptions(max_size=\"11124MB\",patterns=[\"**/*sestra.inp\", \"**/wasim_setup.inp\", \"**/wasim_solve.inp\", \n",
    "                                                                                                    \"**/wasim_snapshots.inp\", \"**/wasim_stru.inp\", \"**/*.csv\", \n",
    "                                                                                                    \"**/*.lis\", \"**/*.lis.bak\", \"**/*.mlg\", \"**/*.sin\", \"**/*.FEM\", \"**/*.jnl\"]),\n",
    "             enable_common_files_copy_to_load_cases=True,\n",
    " )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if analysis is successful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "print(current_time, current_date)\n",
    "\n",
    "#Search different log file types for different messages\n",
    "#Report only not successful log files\n",
    "\n",
    "local__result_path = Path(workspacePath, workflow_client.results_directory)\n",
    "os.chdir(local__result_path)\n",
    "print(\"Verifying Wasim results:\")\n",
    "wasim_succeeded_text = \"FINISHED: SUCCESS\"\n",
    "wasim_files_to_check = {\"Wasim solve\":'wasim_solve.lis.bak', \"Wasim stru\" : 'wasim_stru.lis.bak'}\n",
    "import analysisStatusChecker\n",
    "analysisStatusChecker.checkStatus(wasim_files_to_check,wasim_succeeded_text)\n",
    "wasim_succeeded_text = \"FINISHED: SUCCESS\"\n",
    "wasim_files_to_check = {\"Wasim solve\":'wasim_solve.lis', \"Wasim stru\" : 'wasim_stru.lis'}\n",
    "analysisStatusChecker.checkStatus(wasim_files_to_check,wasim_succeeded_text)\n",
    "\n",
    "print(\"Verifying Sesam Core results:\")\n",
    "score_succeeded_text = \"Duration:\"\n",
    "score_to_check = {\"Sesam Core\":'SCORE.MLG'}\n",
    "analysisStatusChecker.checkStatus(score_to_check,score_succeeded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing\n",
    "Quality checks, maximum usage factors and criterion etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "print(current_time, current_date)\n",
    "\n",
    "def format_time(value, _):\n",
    "    return \"{:.1e}\".format(value)  # Format the time with two decimal places\n",
    "\n",
    "#Loop over all load cases and plot graph of reaction force vs time for each load case\n",
    "for loadcase_folder_name, _ in input_data.iterrows():\n",
    "    args=f\"{prefix_stru}_reactions_history1.csv\" #from Sestra DREA card\n",
    "    args = \"STR__reactions_history1.csv\"\n",
    "    #currently hardcoded name, but should pick up prefix from previous definition\n",
    "    result_path = os.path.join(workspacePath,workflow_client.results_directory, loadcase_folder_name, args)\n",
    "    print(result_path)\n",
    "    reaction_history = pd.read_csv(result_path)\n",
    "    df = pd.read_csv(result_path, delimiter=';') \n",
    "    from matplotlib import pylab as plt\n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "    # Set the default font size for all labels\n",
    "\n",
    "    time = df['Time']\n",
    "    fx = df['FX']\n",
    "    fy = df['FY']\n",
    "    fz = df['FZ']\n",
    "    mx = df['MX']\n",
    "    my = df['MY']\n",
    "    mz = df['MZ']\n",
    "\n",
    "    # Define a custom formatting function for x-axis values\n",
    "   \n",
    "\n",
    "    # Plot a 2D graph\n",
    "    plt.plot(time, fz, marker='.', label='FZ')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Result case id', fontsize=10)\n",
    "    plt.ylabel('Force', fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(format_time))\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.title('Reaction z-force over time-history', fontsize=14)\n",
    "    plt.legend(fontsize=10)  # Show legend with labels\n",
    "\n",
    "    # Add gridlines\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Set x-axis limits\n",
    "    plt.xlim(min(time), max(time))\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(workspacePath,workflow_client.results_directory, loadcase_folder_name,\"reaction_forces_plot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print maximum usage factor and criterion for worst stiffener on each platefield, over all load cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "print(current_time, current_date)\n",
    "\n",
    "panels_and_cases = pd.DataFrame()\n",
    "panel_names = set()\n",
    "for loadcase_folder_name, _ in input_data.iterrows():\n",
    "    result_folder = os.path.join(workspacePath,workflow_client.results_directory, loadcase_folder_name)\n",
    "        \n",
    "    os.chdir(result_folder)\n",
    "\n",
    "    # loop over all buckling result files (csv)\n",
    "    \n",
    "    schema={\n",
    "        \"math score\": int\n",
    "    }\n",
    "    \n",
    "    #.csv file runname is based on name given in SesamCore_buckling_template.jnl on RUN BUCKLING-CHECK line\n",
    "    for file in glob.glob('SesamCore_runname1_*Panel*.csv'):\n",
    "        gen = pd.read_csv(file, dtype=schema, chunksize=10000000)\n",
    "        # Set threshold value for removing the first few result cases to omit initial extreme results\n",
    "        panel = pd.concat((x.query(\"`Result case id` >= 3\") for x in gen), ignore_index=True)\n",
    "        panel['Plate field'] = Path(file).stem\n",
    "        panel['LoadCase'] = loadcase_folder_name\n",
    "        if not panel.empty:\n",
    "            panels_and_cases = pd.concat([panels_and_cases,panel])\n",
    "            panel_names.add(Path(file).stem)\n",
    "\n",
    "#Show table of worst usage factors\n",
    "df2 = pd.DataFrame(columns=['Stiffener name','Plate field', 'UfMax','UfMax criterion','LoadCase', 'Time'])\n",
    "for panel_name in panel_names:\n",
    "    df = panels_and_cases.query('`Plate field` == @panel_name')\n",
    "    if not df.empty:\n",
    "        df2.loc[len(df2.index)] = dict(df.iloc[df['UfMax'].idxmax()])\n",
    "df2.sort_values(by='UfMax',ascending=False,inplace=True)\n",
    "print(f\"Maximum Uf for each panel, taken over all load cases and result cases.\")\n",
    "print(f\"Note that the first two time steps (usually extreme results) have been filtered out to avoid reporting unrealistic peak results.\")\n",
    "display(df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot usage factors for each plate field (all stiffeners) over time-history of worst load case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from ipywidgets import interactive, Text, Dropdown, Layout\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%d/%m/%Y\")\n",
    "print(current_time, current_date)\n",
    "\n",
    "# Number of graphs to display\n",
    "number_of_plate_fields_to_display =10\n",
    "# Result threshold value, e g show the first 10 graphs with maximum usage factors higher than threshold value\n",
    "threshold_for_uf = 1.0 \n",
    "filtered_df = df2[df2['UfMax'] < threshold_for_uf]\n",
    "highet_usage_factor =filtered_df.head(number_of_plate_fields_to_display)\n",
    "plate_fields_with_highest_uf= highet_usage_factor.head(number_of_plate_fields_to_display)[\"Plate field\"].to_list()\n",
    "display(highet_usage_factor)\n",
    "def multiplot(PlateField, LoadCase):\n",
    "    import matplotlib as mpl\n",
    "\n",
    "    # Set the default font size for all labels\n",
    "    mpl.rcParams.update({'font.size': 16})\n",
    "    stiffenernames = data_frames[PlateField]['Stiffener name'].unique()\n",
    "    category_colors = dict(zip(stiffenernames, cm.rainbow(np.linspace(0, 1, len(stiffenernames)))))\n",
    "\n",
    "    ax= data_frames[PlateField].plot(kind='scatter', x='Time', y='UfMax',  c=data_frames[PlateField]['Stiffener name'].apply(lambda x: category_colors[x]), figsize=(20, 7))\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=category)\n",
    "           for category, color in category_colors.items()]\n",
    "    ax.legend(handles=handles, title='Stiffener name')\n",
    "    ax.set_title('UfMax over time-history for worst load case. Inlcudes all time steps, also those omitted in max table reporting.')\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig(\"UfMaxplot.png\")\n",
    "\n",
    "# for loadcase_folder_name, _ in df_cases.iterrows():\n",
    "for loadcase_folder_name in highet_usage_factor['LoadCase'].unique():\n",
    "    result_folder = os.path.join(workspacePath,workflow_client.results_directory, loadcase_folder_name)\n",
    "        \n",
    "    os.chdir(result_folder)\n",
    "    data_frames = {}\n",
    "    for file in glob.glob('SesamCore_runname1_*Panel*.csv'):\n",
    "        try:\n",
    "            data_frames[Path(file).stem] = pd.read_csv(file)[['Time', 'UfMax', 'Stiffener name']]\n",
    "        except:\n",
    "            print(file + \" had issues with formatting\\n\")\n",
    "    \n",
    "    if not data_frames:\n",
    "        print(\"No valid cases found\")\n",
    "    else:\n",
    "        interactive_plot = interactive(multiplot, PlateField=data_frames.keys(), LoadCase=loadcase_folder_name)\n",
    "        display(interactive_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
